{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89ef6b5f-4c07-4c74-a6b8-8fd76e9f7c57",
   "metadata": {},
   "source": [
    "### Implementation of a simple vector space model using tf-idf\n",
    "\n",
    "This is a simple implementation of a vector space model as used in information retrieval using tf-idf. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a114cd1a-2465-49c0-a3d4-b8c68ca4126f",
   "metadata": {},
   "source": [
    "To implement an end-to-end information retrieval system using tf-idf, I followed the following general steps:\n",
    "\n",
    "1. I collected a set of documents that we want to search through. In this case I made a list of around 10000 news articles.\n",
    "\n",
    "2. Then I pre-processed the documnets to remove any irrelevant information e.g. text pre-processing techniques like removing punctuations and tokemization.  \n",
    "\n",
    "3. Next I convert the preprocessed documents into a matrix of tf-idf values. This will involve calculating the term frequency (tf) and inverse document frequency (idf) for each term in each document, and then multiplying the two values to get the tf-idf weight for each term in each document.\n",
    "\n",
    "4. Create a search query that the user will enter to retrieve relevant documents. This query should also be preprocessed in the same way as the documents.\n",
    "\n",
    "5. Lastly, I calculate the similarity between the query and each document using the tf-idf matrix. I use two similarity measures: the dot product of the query and document vector and the cosine similarity. \n",
    "\n",
    "6. Finally, we rank the documents based on their similarity to the query, with the most relevant documents appearing first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7510764b-da8f-4460-9acd-ee4e161bd4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "import csv\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff36176e-77b4-4489-9d74-83a2c6969382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17283</td>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
       "      <td>washington congressional republicans new fear ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17284</td>\n",
       "      <td>Rift Between Officers and Residents as Killing...</td>\n",
       "      <td>After the bullet shells get counted, the blood...</td>\n",
       "      <td>bullet shells get counted blood dries votive c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17285</td>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "      <td>walt disneys bambi opened 1942 critics praised...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17286</td>\n",
       "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
       "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
       "      <td>death may great equalizer isnt necessarily eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17287</td>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
       "      <td>seoul south korea north koreas leader kim said...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  17283  House Republicans Fret About Winning Their Hea...   \n",
       "1  17284  Rift Between Officers and Residents as Killing...   \n",
       "2  17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
       "3  17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
       "4  17287  Kim Jong-un Says North Korea Is Preparing to T...   \n",
       "\n",
       "                                             content  \\\n",
       "0  WASHINGTON  —   Congressional Republicans have...   \n",
       "1  After the bullet shells get counted, the blood...   \n",
       "2  When Walt Disney’s “Bambi” opened in 1942, cri...   \n",
       "3  Death may be the great equalizer, but it isn’t...   \n",
       "4  SEOUL, South Korea  —   North Korea’s leader, ...   \n",
       "\n",
       "                                       clean_content  \n",
       "0  washington congressional republicans new fear ...  \n",
       "1  bullet shells get counted blood dries votive c...  \n",
       "2  walt disneys bambi opened 1942 critics praised...  \n",
       "3  death may great equalizer isnt necessarily eve...  \n",
       "4  seoul south korea north koreas leader kim said...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data we cleaned and saved\n",
    "df = pd.read_csv('train_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c5d2438c-39ea-4215-9144-8bbb8f1a6118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tTHIS DATA COLLECTION HAS\n",
      "\n",
      "\t 99999\t\n",
      "\n",
      "\t DOCUMENTS\n"
     ]
    }
   ],
   "source": [
    "# summary information\n",
    "print(f'\\n\\tTHIS DATA COLLECTION HAS\\n\\n\\t {df.shape[0]}\\t\\n\\n\\t DOCUMENTS')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115fe185-2548-4cbf-9ba6-c3c87853f86f",
   "metadata": {},
   "source": [
    "### Pre-process the data\n",
    "Here we tokenize, stem and clean the text off all random noise so that we can use it to build the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b98256-2e92-4a20-9a72-bcf1d9f35f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time # takes a while to run\n",
    "documents = list(df['content'].values)\n",
    "\n",
    "def preprocess_document(document):\n",
    "    # Tokenize the document into individual words\n",
    "    words = nltk.word_tokenize(document)\n",
    "    \n",
    "    # Remove stop words (common words that do not provide useful information)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Apply stemming to reduce words to their base forms\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "# preprocess the documents by tokenizing and removing stopwords\n",
    "processed_docs = []\n",
    "for doc in documents:\n",
    "    \n",
    "    tokens = preprocess_document(doc)\n",
    "    \n",
    "    filtered_tokens1 = [token for token in tokens if token not in stopwords.words(\"english\")]\n",
    "    \n",
    "    processed_docs.append(\" \".join(filtered_tokens1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e710c93-db39-43d7-bb81-45e1d6a90898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv foe easier access -- RuN only once\n",
    "df['clean_content'] = processed_docs\n",
    "df.to_csv('train_data2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8638815e-6ae9-4004-9b92-e2d8e145c0fd",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a9d37ba-6c7b-4268-8b42-4550fbd5c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-processed data\n",
    "df = pd.read_csv('train_data.csv')\n",
    "\n",
    "# using fillna() function\n",
    "df = df.fillna('')\n",
    "\n",
    "processed_docs = list(df['clean_content'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9bbe8504-4f24-478a-92b5-2a6504c7fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vector space model using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(processed_docs)\n",
    "\n",
    "# save an instance of the model to a file\n",
    "pickle.dump(X, open(\"tfidf.pickle\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6396faa8-a8a3-4171-8d1b-282ac7445621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a search query:\n",
      "  The ethics of machine learning and state of artificial intelligence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Querry...please wait..\n",
      "\n",
      "------------------------------\n",
      "Displaying search results\n",
      "\n",
      "DocumentID: 75603\n",
      "Rank: 1\n",
      "Title of Retrieved Document 1: Amazon Is Making It Easier for Companies to Track You\n",
      "Similarity to query: 0.3805\n",
      "\n",
      "Summary of Retrieved Document:\n",
      "---------------------\n",
      "   Like “big data” and “social media” before it, the term “artificial intelligence” has become so buzzworthy at this point that it’s largely lost meaning. If everything seems to be powered by A. I. that’s because many companies are desperate to be perceived as leaders in machine learning (or deep learning, or natural language generation, all of which fall under the A. I. umbrella) —  even when they’re not. Artificial intelligence is an increasingly powerful force in the world, even as our grasp of what A. I. is and does continuously evolves. ” So it makes sense that Jeff Bezos, the Amazon founder and CEO, spent a good chunk of his latest letter to Amazon shareholders focused on artificial intelligence. It contains of all kinds of pleasing Bezosisms —  “disagree and commit” deserves its own article, really —  but I want to focus on the section Bezos devotes to artificial intelligence, which he describes as a big trend but one that’s also “strangely hard for large organizations to embrace. ” “At Amazon, we’ve been engaged in the practical application of machine learning for many years now,” Bezos writes. “Some of this work is highly visible: our autonomous Prime Air delivery drones the Amazon Go convenience store that uses machine vision to eliminate checkout lines and Alexa, our   AI assistant. ” Here’s where it starts to get more interesting: “But much of what we do with machine learning happens beneath the surface. Machine learning drives our algorithms for demand forecasting, product search ranking, product and deals recommendations, merchandising placements, fraud detection, translations, and much more. ” And then, when Bezos gets into what all this means for Amazon Web Services, Amazon’s cloud services platform, is where it gets really interesting: “Inside AWS, we’re excited to lower the costs and barriers to machine learning and AI so organizations of all sizes can take advantage of these advanced techniques,” Bezos writes. He goes on to describe how Amazon’s   clients can use the company’s     frameworks —  including the systems that power the Amazon Echo Amazon Polly, the company’s    program and Amazon Rekognition, its facial recognition software. Clients have access to these technologies through a simple API, Bezos says, meaning developers for a range of companies can tap into Amazon’s suite of A. I. programs without having any machine learning expertise themselves. Mainly, because it means Amazon is enabling countless organizations to track its users more precisely than ever. (That’s why, when AWS has a server problem, it seems like the entire internet is coming apart at the seams.) Of course, many of the biggest companies that use Amazon Web Services already run in their own     operations. (Remember, AWS clients include McDonald’s, Netflix, Airbnb, Adobe, Capital One, GE, and Pinterest, to name a few.) It was in Bezos’s shareholder letter last year that he boasted of AWS’s stunning client base: “more than a million customers from organizations of every size across nearly every industry,” he wrote. ” All this is a reminder that the   conceptions of how businesses use artificial intelligence —  the device sitting in your kitchen that responds to your voice, or the drone dropping an Amazon package on your doorstep —  are only a teensy slice of what a company means when it says it’s using A. I. Instead, much of the corporate world’s algorithmic   can be boiled down to learning exactly who customers are and how they behave. “Though less visible, much of the impact of machine learning will be of this type,” Bezos says in his letter, “quietly but meaningfully improving core operations. ” “Watch this space,” he adds. But we’re not likely to learn very much more by simply watching, when so much of what artificial intelligence does is invisible from the outside.\n",
      "\n",
      "\n",
      "\n",
      "DocumentID: 31087\n",
      "Rank: 2\n",
      "Title of Retrieved Document 2: Tech Billionaires Create Fund to Prevent Robot Apocalypse - Breitbart\n",
      "Similarity to query: 0.32\n",
      "\n",
      "Summary of Retrieved Document:\n",
      "---------------------\n",
      " [LinkedIn Founder Reid Hoffman and eBay Founder Pierre Omidyar’s Omidyar Network have each donated $10 million dollars to the “Ethics and Governance of Artificial Intelligence Fund. ” The Knight Foundation has contributed $5 million to the cause, while Raptor Group founder Jim Pallotta and the William and Flora Hewlett Foundation have made respective donations of $1 million. As the Knight Foundation explains: Even when we don’t know it, artificial intelligence affects virtually every aspect of our modern lives. Yet, for something so influential, there’s an odd assumption that artificial intelligence agents and machine learning, which enable computers to make decisions like humans and for humans, is a neutral process. The Ethics and Governance of Artificial Intelligence Fund will allocate portions of its $27 million to people dedicated to addressing this issue, and the ever more complicated questions that will arise as we move toward a future guided in large part by the intelligent constructs that already know our names, our addresses, our incomes, our children, and even our favorite foods. Harvard’s Berkman Klein Center and the MIT Media Lab will serve as “founding anchor institutions” in an effort at “bridging the gap between the humanities, the social sciences, and computing by addressing the global challenges of artificial intelligence (AI) from a multidisciplinary perspective. ” MIT Media Lab Director Joi Ito outlined the “tough challenges” that “AI’s rapid development” brings with it: For example, one of the most critical challenges is how do we make sure that the machines we ‘train’ don’t perpetuate and amplify the same human biases that plague society? According to the MIT Media Lab statement, the fund will be managed by “a small board, consisting of leadership from each participating foundation and institution,” as well as members of their faculty and “a number of other individuals from a wide range of disciplines and organizations. ” With the concept of truly intelligent artificial beings climbing even as far as the European parliament, this research seems especially relevant. According to the Berkman Klein Center, the participating organizations “welcome public engagement,” but are not currently seeking further investment from the general public. They do, however, “welcome discussions with all institutions and individuals engaging in research related to developing ethical AI in the public interest.\n",
      "\n",
      "\n",
      "\n",
      "DocumentID: 71384\n",
      "Rank: 3\n",
      "Title of Retrieved Document 3: Apple is facing a crisis of salesmanship\n",
      "Similarity to query: 0.3021\n",
      "\n",
      "Summary of Retrieved Document:\n",
      "---------------------\n",
      " As smartphone innovation seems to have plateaued, the tech giants of the world, notably  and  have doubled down on machine learning and artificial intelligence  —   the trendy technology that’s making for .” ”It’s a big, necessary step for the industry. ” ’’ ”With the hardware unexciting at best, that means that the onus will be on Apple to prove that the iPhone is differentiated from Google’s   Android elsewhere. Namely, it must prove the upcoming  has game with the new machine learning trend and it will bring intelligence to the whole iPhone.” ’’ ’For Apple, the peril is twofold.’ ”First, Wall Street is afraid that we’ve reached  and it’s all downhill from here. Second, customers and analysts alike are concerned that after years of   iPhone releases and the failure of new products like the  and  to light the market ablaze, Apple’s ability to innovate has peaked, too.” ”That’s why Apple’s PR machine spent much of August in overdrive, with top company execs including Tim Cook, Eddy Cue, and Phil Schiller giving interviews to  The  and .” ”In each interview, the content may have varied, but the message was always the same: If you think Apple’s glory days are behind it, think again.” ’The coded message is: Apple is not behind in new technologies like machine learning.’ ’’ ’Instead, Apple execs explained to that there is indeed an ”Apple brain” on every iPhone and iPad that learns from user behavior. Apple sees it as part of that overall, signature    experience, rather than a total revolution.’ ’’ ’’ ’’ ’’ ’But that just underscores the struggle of selling the stuff that machine learning makes possible.’ ”Many of the coolest things it enables, from a technical standpoint   —   better app recommendations, facial recognition in photos, speech recognition, fraud prevention and security  —   are nifty and useful, but also the kind of things you tend to only ever notice when it doesn’t work.” ’’ ”Which is why you’ve heard so much from Apple about the Siri voice assistant and the new smarts that she’s getting in iOS 10. This is the thing you can use every day to make your life better.” ’It remains to be seen if the   Siri will be enough to reverse user behavior, given that surveys have found that . Siri, with her new smarts, becomes what’s essentially a mascot for the   Apple Brain, more so than she already is.” ”She’s the most tangible example of what machine learning can do, even if she’s not necessarily the best or most useful.” ’’ ’The exact same factors are going into  and the forthcoming  too.’ ”So don’t be surprised if Apple starts talking up Siri as better than all other smart assistants.\n",
      "\n",
      "\n",
      "\n",
      "DocumentID: 71054\n",
      "Rank: 4\n",
      "Title of Retrieved Document 4: You don’t need to have a computer science degree from Stanford to be working on one of Google’s hottest teams\n",
      "Similarity to query: 0.2956\n",
      "\n",
      "Summary of Retrieved Document:\n",
      "---------------------\n",
      " ’ ’   The team held a question and answer session yesterday on Reddit,   and one of the most striking parts (to someone not entrenched in   that world, at least) was reading about the     backgrounds that Google Brain team members   have. ’ ”   You might think that to be working at one of the preeminent   machine learning groups, you would have to have a degree in   computer science from Stanford. ” ’’ ’   Here are some of surprising paths that Google Brain   employees have taken: ’ ’   ”Machine Learning is such a new field though that degrees matter   less than you might think,” . ”I think all you   really need to get started is a college level foundation in   Vector Calculus & Linear Algebra, plus proficiency in Python,   C++ or similar.” ’ ”   Brain’s newly launched Residency Program specifically looks for   people with different specialties to go through a  that dives into deep   learning techniques. ”We believe that mixing different   perspectives and types of expertise can spark creative new ideas   and facilitate closer collaborations with other fields.” ’ ’   Read the rest of the  ’\n",
      "\n",
      "\n",
      "\n",
      "DocumentID: 69681\n",
      "Rank: 5\n",
      "Title of Retrieved Document 5: Here are the most exciting things Google announced at its giant conference\n",
      "Similarity to query: 0.2853\n",
      "\n",
      "Summary of Retrieved Document:\n",
      "---------------------\n",
      " ”During the  Google CEO Sundar Pichai focused on Google’s plans to bake artificial intelligence and machine learning more thoroughly into all of its services.” ’In that vein, the company also unveiled a bunch of new products, including two messaging apps and a smart speaker.’ ”Here’s what caught our eye:” ”During the keynote of its   developers’.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the file\n",
    "with open(\"tfidf.pickle\", \"rb\") as file:\n",
    "    saved_tfidf = pickle.load(file)\n",
    "\n",
    "\n",
    "# define a sample query\n",
    "query = input(\"\\nEnter a search query:\\n \")\n",
    "\n",
    "print('Processing Querry...please wait..\\n')\n",
    "print('---'*10)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "print('Displaying search results\\n')\n",
    "\n",
    "# preprocess the query by tokenizing and removing stopwords\n",
    "tokens = word_tokenize(query)\n",
    "\n",
    "filtered_tokens = [token for token in tokens if token not in stopwords.words(\"english\")]\n",
    "\n",
    "query = \" \".join(filtered_tokens)\n",
    "\n",
    "# transform the query using the same vector space model\n",
    "query_vector = vectorizer.transform([query])\n",
    "\n",
    "# calculate the cosine similarity between the query vector and the document vectors\n",
    "similarity = (saved_tfidf * query_vector.T).A\n",
    "\n",
    "df2 = df.copy()\n",
    "df2['similarity_measure'] = similarity.flatten()\n",
    "results = df2.sort_values(by='similarity_measure',ascending=False)\n",
    "top_5 = results.head()\n",
    "\n",
    "\n",
    "# summarize the content of the top 5 docs\n",
    "# Input text - to summarize \n",
    "text = list(top_5['content'].values)\n",
    "\n",
    "# def summarize(text):\n",
    "i = 1\n",
    "\n",
    "for text in text:\n",
    "    \n",
    "    title = top_5.iloc[i-1]['title'].strip()\n",
    "    doc_id = top_5.iloc[i-1]['id']\n",
    "    similarity = round(top_5.iloc[i-1]['similarity_measure'], 4)\n",
    "    \n",
    "    # Tokenizing the text\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Creating a frequency table to keep the score of each word\n",
    "    freqTable = dict()\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in stopWords:\n",
    "            continue\n",
    "        if word in freqTable:\n",
    "            freqTable[word] += 1\n",
    "        else:\n",
    "            freqTable[word] = 1\n",
    "\n",
    "    # Creating a dictionary to keep the score\n",
    "    # of each sentence\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentenceValue = dict()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for word, freq in freqTable.items():\n",
    "            if word in sentence.lower():\n",
    "                if sentence in sentenceValue:\n",
    "                    sentenceValue[sentence] += freq\n",
    "                else:\n",
    "                    sentenceValue[sentence] = freq\n",
    "\n",
    "\n",
    "    sumValues = 0\n",
    "    for sentence in sentenceValue:\n",
    "        sumValues += sentenceValue[sentence]\n",
    "\n",
    "    # Average value of a sentence from the original text\n",
    "    average = int(sumValues / len(sentenceValue))\n",
    "\n",
    "    # Storing sentences into our summary.\n",
    "    summary = ''\n",
    "    for sentence in sentences:\n",
    "        if (sentence in sentenceValue) and (sentenceValue[sentence] > (1 * average)):\n",
    "            summary += \" \" + sentence\n",
    "        \n",
    "    print(f'DocumentID: {doc_id}')\n",
    "    print(f'Rank: {i}')\n",
    "    print(f'Title of Retrieved Document {i}: {title}')\n",
    "    print(f'Similarity to query: {similarity}\\n')\n",
    "    print('Summary of Retrieved Document:')\n",
    "    print('---'*7)\n",
    "    print(summary)\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
